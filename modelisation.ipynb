{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effectuer les traitements nécessaires pour les variables catégorielles**   \n",
    "❒ Choisir l'encodage pertinent (one hot encoding, label encoding, autre)   \n",
    "❒ Filtrer les catégories présentes sous un certain seuil dans le dataset pour réduire la dimension   \n",
    "\n",
    "**Mettre en oeuvre des techniques de réduction de dimension si nécessaire**   \n",
    "❒ Appliquer une technique de réduction de dimension si besoin (exemple : Ne pas faire de PCA (ou autres) pour réduire la dimensions si les genres seulement sont binairisés)   \n",
    "\n",
    "**Utiliser un modèle d'apprentissage non supervisé approprié**   \n",
    "❒ Utiliser une distance pertinente en fonction de la nature des données, éventuellement plusieurs distances moyennées pour différents groupes de variables si elles ne sont pas du même type   \n",
    "❒ si un algorithme de clustering est utilisé, le paramétriser de façon efficace (si kmeans, k doit être assez grand (pas 3), mais pas trop (clusters avec moins de 5 éléments)), ou hierarchical clustering pour avoir des granularités variables \n",
    "❒ Ou tester de prendre les N films les plus proches   \n",
    "❒ essayer à la fois une approche de clustering et une approche de voisins, discuter des avantages et incovénients de chaque méthode dans le cadre du projet   \n",
    "\n",
    "**Évaluer les performances d’un modèle d’apprentissage non supervisé**   \n",
    "❒ Si critère basé sur des comparaisons de distance intra/inter clusters, ne pas comparer des distances incomparables (pas définies sur le même set de variables, dans la même dimension...)   \n",
    "❒ Avoir une approche empirique raisonnable : prendre des films variés pour avoir une vision globale   \n",
    "❒ Essayer d'élaborer les appréciations des performances (ne pas se contenter d'un commentaire simpliste)   \n",
    "❒ réfléchir à ce qu'est une bonne recommandation : a-t-on besoin de ML pour regarder harry potter 4 après le 3   \n",
    "❒ essayer de quanitifer les performances (réfléchir à, voire mettre en oeuvre un système de notation des différentes approches)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "import calendar;\n",
    "import time;\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from IPython.display import Audio\n",
    "sound_file = 'rossignol.wav'\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_cleaned.csv', index_col = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4917, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>Color</td>\n",
       "      <td>Ron Shelton</td>\n",
       "      <td>71.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>Dash Mihok</td>\n",
       "      <td>556.0</td>\n",
       "      <td>9059588.0</td>\n",
       "      <td>Crime|Drama|Romance|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>R</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.35</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>Color</td>\n",
       "      <td>Michael Winterbottom</td>\n",
       "      <td>190.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>Archie Panjabi</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>9172810.0</td>\n",
       "      <td>Biography|Drama|History|Thriller|War</td>\n",
       "      <td>...</td>\n",
       "      <td>118.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>R</td>\n",
       "      <td>16000000.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.35</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>Color</td>\n",
       "      <td>David Raynr</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>Jodi Lyn O'Keefe</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>8735529.0</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.85</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>Color</td>\n",
       "      <td>Mort Nathan</td>\n",
       "      <td>63.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>Lin Shaye</td>\n",
       "      <td>890.0</td>\n",
       "      <td>8586376.0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>...</td>\n",
       "      <td>132.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>R</td>\n",
       "      <td>20000000.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>852.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       color         director_name  num_critic_for_reviews  duration  \\\n",
       "index                                                                  \n",
       "0      Color         James Cameron                   723.0     178.0   \n",
       "2705   Color           Ron Shelton                    71.0     118.0   \n",
       "2706   Color  Michael Winterbottom                   190.0     108.0   \n",
       "2707   Color           David Raynr                    50.0      94.0   \n",
       "2708   Color           Mort Nathan                    63.0      97.0   \n",
       "\n",
       "       director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "index                                                                      \n",
       "0                          0.0                   855.0  Joel David Moore   \n",
       "2705                      41.0                   274.0        Dash Mihok   \n",
       "2706                     187.0                   254.0    Archie Panjabi   \n",
       "2707                       9.0                   612.0  Jodi Lyn O'Keefe   \n",
       "2708                       2.0                   461.0         Lin Shaye   \n",
       "\n",
       "       actor_1_facebook_likes        gross  \\\n",
       "index                                        \n",
       "0                      1000.0  760505847.0   \n",
       "2705                    556.0    9059588.0   \n",
       "2706                  11000.0    9172810.0   \n",
       "2707                  11000.0    8735529.0   \n",
       "2708                    890.0    8586376.0   \n",
       "\n",
       "                                     genres         ...           \\\n",
       "index                                               ...            \n",
       "0           Action|Adventure|Fantasy|Sci-Fi         ...            \n",
       "2705           Crime|Drama|Romance|Thriller         ...            \n",
       "2706   Biography|Drama|History|Thriller|War         ...            \n",
       "2707                   Comedy|Drama|Romance         ...            \n",
       "2708                                 Comedy         ...            \n",
       "\n",
       "      num_user_for_reviews language  country  content_rating       budget  \\\n",
       "index                                                                       \n",
       "0                   3054.0  English      USA           PG-13  237000000.0   \n",
       "2705                 125.0  English      USA               R   15000000.0   \n",
       "2706                 118.0  English      USA               R   16000000.0   \n",
       "2707                  89.0  English      USA           PG-13   15000000.0   \n",
       "2708                 132.0  English      USA               R   20000000.0   \n",
       "\n",
       "       title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
       "index                                                               \n",
       "0          2009.0                  936.0        7.9          1.78   \n",
       "2705       2002.0                  463.0        6.6          2.35   \n",
       "2706       2007.0                  883.0        6.7          2.35   \n",
       "2707       2000.0                  897.0        5.5          1.85   \n",
       "2708       2002.0                  852.0        4.9          1.85   \n",
       "\n",
       "      movie_facebook_likes  \n",
       "index                       \n",
       "0                    33000  \n",
       "2705                   455  \n",
       "2706                   923  \n",
       "2707                   816  \n",
       "2708                  1000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Effectuer les traitements nécessaires pour les variables catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Choisir l'encodage pertinent (one hot encoding, label encoding, autre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour director_name je vais utiliser OneHotEncoder\n",
    "Pour les catégories à plusieurs labels séparés par un |, je vais utiliser TfidfVectorizer parce qu'il me permet d'utiliser une fonction tokenize qui sait isoler les différentes catégories séparées par un |.\n",
    "avec use_idf = False pour rapporter la fréquence du terme au film seulement (3 termes = fréquence de 0,33 chacun) et non pas à l'ensemble des films.   \n",
    "\n",
    "je préfère cet encoding à l'encoding LabelEncoder qui introduit des proximités artificielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokenizer(s):\n",
    "   return s.split('|')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = split_tokenizer, use_idf = False)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot = OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Filtrer les catégories présentes sous un certain seuil dans le dataset pour réduire la dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de transformer les variables catégorielles en dummy colonnes, je réduit le nombre de colonnes résultats en ne conservant que les catégories les plus fréquentes (eg.qui apparaissent pour au moins 6 films) pour director_name, actor_n_name et plot_keywords.    \n",
    "Je définis donc des fonctions helper pour ces traiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J'ai recopié cette fonction trouvée dans un ce kernel Kaggle : https://www.kaggle.com/fabiendaniel/film-recommendation-engine\n",
    "# car la trouve très pratique. Comme elle est codée très compacte, je l'ai documentée pour faciliter sa compréhension.\n",
    "\n",
    "def count_word(df, ref_col, liste):\n",
    "\n",
    "#paramètres :\n",
    "#df : la dataframe\n",
    "#col : la colonne\n",
    "#liste : vocabulaire complet (ou restreint) pour la colonne considérée = liste des valeurs uniques prises par la colonne, \n",
    "#après séparation par split \"|\"\n",
    "\n",
    "#variables intermédiaires\n",
    "#liste_words : liste des mots trouvés dans une colonne après séparation par '|'\n",
    "\n",
    "    #initialisation d'un dictionnaire compteur avec une entrée par mot du vocabulaire\n",
    "    # ce dictionnaire contiendra le nb d'occurences du mot dans la dataframe\n",
    "    word_count = dict()\n",
    "    for s in liste: word_count[s] = 0\n",
    "    \n",
    "    # liste des mots dans la colonne\n",
    "    for liste_words in df[ref_col].str.split('|'):        \n",
    "        \n",
    "        # si la valeur dans la colonne n'est pas du texte (image_ratio par exemple)\n",
    "        if type(liste_words) == float and pd.isnull(liste_words): continue   #pas compris \"and pd.isnull(liste_words)\"\n",
    "        # si un mot de la colonne est présent dans la liste entrée en paramètre de la fonction, on augmente le compteur\n",
    "        for s in [s for s in liste_words if s in liste]: \n",
    "            if pd.notnull(s): word_count[s] += 1\n",
    "    #______________________________________________________________________\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    word_occurences = []\n",
    "    for w,v in word_count.items():\n",
    "        word_occurences.append([w,v])\n",
    "    word_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return word_occurences, word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction retourne la liste des valeurs d'une catégorie utilisées au moins 6 fois dans le dataset\n",
    "# le nombre de 6 est relativement arbitraire car on pourrait recommander 4 films avec un directeur commun \n",
    "# et 2 films avec un acteur commun. \n",
    "\n",
    "def frequent_categories(df, column, n=0) :\n",
    "\n",
    "    category_set = df[column].unique()\n",
    "    category_occurences, dum = count_word(df, column, category_set)\n",
    "    df_category_occurences = pd.DataFrame(category_occurences, columns = ['word', 'count'])\n",
    "    frequent_categories = df_category_occurences[df_category_occurences['count'] > n ]['word'].unique()\n",
    "    return list(frequent_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_director(n_frequency=0) :\n",
    "    \n",
    "    frequent_directors = frequent_categories(df, 'director_name', n_frequency)\n",
    "    frequent_directors\n",
    "    \n",
    "    # **Je remplace les directeurs peu fréquents par la valeur None**    \n",
    "    df[\"director_name\"] = df[\"director_name\"].apply(\n",
    "        lambda x: x  if x in (frequent_directors) else None)\n",
    "    \n",
    "    #**J'encode avec OneHot**\n",
    "    encoded_director =  one_hot.fit_transform(df[[\"director_name\"]].values.astype('U'))\n",
    "    return encoded_director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x2398 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4917 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_director = pre_process_director()\n",
    "encoded_director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 acteurs\n",
    "**Je vais regrouper les acteurs pour éviter qu'un fim B soit considéré comme non similaire à un film A parce que Julia Roberts est en acteur 2 dans l'un et en acteur 1 dans l'autre**\n",
    "\n",
    "**auparavant je ne conserve que  les acteurs cités dans au moins 6 films, soit comme acteur 1, soit acteur 2, soit acteur 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_actors(n_frequency=0) :\n",
    "# liste globale des acteurs cités au moins n fois\n",
    "    frequent_actors = set()\n",
    "    frequent_actor_1_name = frequent_categories(df, 'actor_1_name',n_frequency)\n",
    "    frequent_actors.update(frequent_actor_1_name)\n",
    "    frequent_actor_2_name = frequent_categories(df, 'actor_2_name', n_frequency)\n",
    "    frequent_actors.update(frequent_actor_2_name)\n",
    "    frequent_actor_3_name = frequent_categories(df, 'actor_3_name', n_frequency)\n",
    "    frequent_actors.update(frequent_actor_3_name)\n",
    "    frequent_actors = set(frequent_actors)\n",
    "\n",
    "    #Je remplace les valeurs rares par None\n",
    "    df[\"actor_1_name\"] = df[\"actor_1_name\"].apply(\n",
    "        lambda x: x  if x in (frequent_actors) else '')\n",
    "    df[\"actor_2_name\"] = df[\"actor_2_name\"].apply(\n",
    "        lambda x: x  if x in (frequent_actors) else '')\n",
    "    df[\"actor_3_name\"] = df[\"actor_3_name\"].apply(\n",
    "        lambda x: x  if x in (frequent_actors) else '')\n",
    "    \n",
    "    # Je regroupe les acteurs\n",
    "    df['actors'] = df['actor_1_name'] + '|' + df['actor_2_name']  + '|' + df['actor_3_name']\n",
    "    \n",
    "    # J'encode avec TFiDF\n",
    "    encoded_actors = tfidf_vectorizer.fit_transform(df['actors'].values.astype('U'))\n",
    "    \n",
    "    return encoded_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x6252 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14731 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_actors = pre_process_actors()\n",
    "encoded_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Plot_keywords\n",
    "**Je vais encoder par Tfidf seulement les mots clés utilisés pour au moins n films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_keywords(n_frequency=0) :\n",
    "    # Calcul des occurrence des mots clés\n",
    "    keywords_set = set()\n",
    "\n",
    "    for string in df[df.plot_keywords.notnull()]['plot_keywords'].str.split('|').values:\n",
    "        keywords_set = keywords_set.union(set(string))\n",
    "    \n",
    "    keyword_occurences, dum = count_word(df, 'plot_keywords', keywords_set)\n",
    "    # identification des mots clés utilisés moins de 6 fois, pour les exclure de l'encodinf\n",
    "\n",
    "    df_keyword_occurences = pd.DataFrame(keyword_occurences, columns = ['word', 'count'])\n",
    "    stop_keywords = list(df_keyword_occurences[df_keyword_occurences['count'] <n_frequency ]['word'])\n",
    "    #stop_keywords\n",
    "    \n",
    "    # J'encode avec Tfidf\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer = split_tokenizer, stop_words = stop_keywords, use_idf = False)\n",
    "    encoded_keywords = tfidf_vectorizer.fit_transform(df['plot_keywords'].values.astype('U'))\n",
    "    \n",
    "    return encoded_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x8086 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23639 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_keywords = pre_process_keywords()\n",
    "encoded_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On pourrait réduire ce nombre en utilisant une lemmisation (mots partageant une même racine) ou un word-embedding (réduction de dimension grâce à une réprésenttaion sémentique du vocabulaire)** Je ne l'ai pas fait pour ce projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Genres\n",
    "**Je vais encoder en Tfidf les genres cités pour au moins n films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_genres(n_frequency=0) :\n",
    "    genres_set = set()\n",
    "\n",
    "    for string in df[df.genres.notnull()]['genres'].str.split('|').values:\n",
    "        genres_set = genres_set.union(set(string))\n",
    "    \n",
    "    genres_occurences, dum = count_word(df, 'genres', genres_set)\n",
    "    # identification des mots clés utilisés moins de n fois, pour les exclure de l'encodinf\n",
    "\n",
    "    df_genres_occurences = pd.DataFrame(genres_occurences, columns = ['word', 'count'])\n",
    "    stop_genres = list(df_genres_occurences[df_genres_occurences['count'] <n_frequency ]['word'])\n",
    "    #stop_keywords\n",
    "    \n",
    "    # J'encode avec Tfidf\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer = split_tokenizer, stop_words = stop_genres, use_idf = False)\n",
    "    encoded_genres = tfidf_vectorizer.fit_transform(df['genres'].values.astype('U'))\n",
    "    \n",
    "    return encoded_genres\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x26 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14126 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_genre = pre_process_genres()\n",
    "encoded_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 content-rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Je fais un encodage manuel ordonné de ma jière croissante du film grand public jusqu'au film le plus restreint**\n",
    "J'ai regardé les définitions des content-rating sur wikipedia, et j'en ai conclu qu'ils pouvaient être ordonnés de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# J'encode avec un encodage manuel\n",
    "def content_rating_encoded(label) :\n",
    "    if label in('G', 'T-Y', 'TV-G', 'Unrated', 'Non rated' , 'passed', 'approved', 'rated', 'non rated') : \n",
    "        x=int(1)\n",
    "    elif label in ('PG', 'GP') :  \n",
    "        x = int(2)\n",
    "    elif label in ('PG-13', 'TV-14') :\n",
    "        x = int(3)\n",
    "    elif label in ('R', 'X') :\n",
    "        x = int(4)\n",
    "    elif label in('NC-17', 'TV-MA') :\n",
    "        x = int(5)\n",
    "    else : \n",
    "        x = int(1)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def pre_process_content_rating(n_frequency=0) :\n",
    "    \n",
    "    df[\"content_rating\"] = df[\"content_rating\"].apply(\n",
    "        lambda x: content_rating_encoded(x))\n",
    "    \n",
    "    return df[\"content_rating\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_rating\"] = pre_process_content_rating()\n",
    "#df[\"content_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_content_rating = pre_process_content_rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_content_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 language, country\n",
    "\n",
    "**Je vais réaliser un encoding One Hot en ne conservant que les pays et langauges utilisés dans au moins 6 films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_country(n_frequency=0) :\n",
    "    frequent_countries = frequent_categories(df, 'country', n_frequency)\n",
    "    df[\"country\"] = df[\"country\"].apply(\n",
    "        lambda x: x  if x in (frequent_countries) else None)\n",
    "    encoded_country =  one_hot.fit_transform(df[['country']].values.astype('U'))\n",
    "    return encoded_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x65 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4917 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_country = pre_process_country()\n",
    "encoded_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_language(n_frequency=0) :\n",
    "    frequent_languages = frequent_categories(df, 'language', n_frequency)\n",
    "    df[\"language\"] = df[\"language\"].apply(\n",
    "        lambda x: x  if x in (frequent_languages) else None)\n",
    "    encoded_language =  one_hot.fit_transform(df[['language']].values.astype('U'))\n",
    "    return encoded_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4917x47 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4917 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_language = pre_process_language()\n",
    "encoded_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Normalisation des valeurs numériques\n",
    "\n",
    "Je vais normaliser entre 0 et 1, qui est l'intervalle dans lequel se trouvent déjà les valeurs des dummy variables catégorielles.\n",
    "Dois-je standardiser (centrer-réduire) ? Si j'ai bien compris, je pense que non. En effet à part la note Imdb, toutes ces variables ont des distributions très différentes d'une distribution gaussienne. Je pense que la standardisation transformerait trop les données.\n",
    "\n",
    "Avant de normaliser je vais traiter les outliers. J'en ai trouvé 1 : le budget le plus élevé, qui n'est pas une valeur aberrante mais qui fausserait le résultat de la normalisation MinMax.\n",
    "\n",
    "Je vais remplacer cette valeur par une valeur légèrement au dessus (+ 10%) de la 2ème valeur la plus forte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1_budget = df.sort_values(by = 'budget', ascending = False)['budget'].values[0]\n",
    "max2_budget = df.sort_values(by = 'budget', ascending = False)['budget'].values[1]\n",
    "mask = df.budget == max1_budget\n",
    "column_name = 'budget'\n",
    "df.loc[mask, column_name] = max2_budget * 1.1\n",
    "\n",
    "#df.loc[[2988]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def pre_process_numeric_variables() :\n",
    "    \n",
    "    numeric_columns = [\n",
    "    'duration', 'director_facebook_likes', 'actor_3_facebook_likes', 'actor_1_facebook_likes', \\\n",
    "    'gross', 'num_voted_users', 'cast_total_facebook_likes', 'facenumber_in_poster', \\\n",
    "    'num_user_for_reviews','budget', 'title_year', 'actor_2_facebook_likes', 'imdb_score', \\\n",
    "    'movie_facebook_likes', 'content_rating', 'aspect_ratio'\n",
    "    ]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df[numeric_columns] = pd.DataFrame(scaler.fit_transform(df[numeric_columns]))\n",
    "    \n",
    "    #l'encodage MinMax des vaeurs numériques a produit des valeurs nan que je remplace par la valeur moyenne de la colonne\n",
    "    \n",
    "    imp_median = SimpleImputer(strategy='median')\n",
    "\n",
    "    df[numeric_columns] = imp_median.fit_transform(df[numeric_columns])\n",
    "    df_numeric = df[numeric_columns].to_sparse(fill_value=0)\n",
    "    \n",
    "    return df_numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>cast_total_facebook_likes</th>\n",
       "      <th>facenumber_in_poster</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>aspect_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524453</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603479</td>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.104708</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162680</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.036739</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.020313</td>\n",
       "      <td>0.057103</td>\n",
       "      <td>0.055327</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.077683</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.189553</td>\n",
       "      <td>0.043993</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.299755</td>\n",
       "      <td>0.160725</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461356</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  director_facebook_likes  actor_3_facebook_likes  \\\n",
       "index                                                              \n",
       "0      0.339286                 0.000000                0.037174   \n",
       "2705   0.309524                 0.608696                0.017913   \n",
       "2706   0.267857                 0.036739                0.034217   \n",
       "2707   0.154762                 0.003652                0.025870   \n",
       "2708   0.196429                 0.000000                0.010522   \n",
       "\n",
       "       actor_1_facebook_likes     gross  num_voted_users  \\\n",
       "index                                                      \n",
       "0                    0.001563  1.000000         0.524453   \n",
       "2705                 0.001164  0.062299         0.104708   \n",
       "2706                 0.020313  0.057103         0.055327   \n",
       "2707                 0.001563  0.189553         0.043993   \n",
       "2708                 0.001109  0.299755         0.160725   \n",
       "\n",
       "       cast_total_facebook_likes  facenumber_in_poster  num_user_for_reviews  \\\n",
       "index                                                                          \n",
       "0                       0.007361              0.000000              0.603479   \n",
       "2705                    0.003851              0.000000              0.162680   \n",
       "2706                    0.025246              0.023256              0.077683   \n",
       "2707                    0.006344              0.000000              0.019174   \n",
       "2708                    0.002551              0.000000              0.461356   \n",
       "\n",
       "         budget  title_year  actor_2_facebook_likes  imdb_score  \\\n",
       "index                                                             \n",
       "0      0.051299        0.93                0.006832    0.797468   \n",
       "2705   0.015151        0.89                0.003547    0.759494   \n",
       "2706   0.015151        0.87                0.007299    0.632911   \n",
       "2707   0.015476        0.82                0.007299    0.468354   \n",
       "2708   0.015584        0.86                0.003861    0.645570   \n",
       "\n",
       "       movie_facebook_likes  content_rating  aspect_ratio  \n",
       "index                                                      \n",
       "0                  0.094556             0.0      0.040486  \n",
       "2705               0.000000             0.0      0.078947  \n",
       "2706               0.000000             0.0      0.078947  \n",
       "2707               0.001989             0.0      0.045209  \n",
       "2708               0.000000             0.0      0.045209  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numeric = pre_process_numeric_variables()\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je place dans mon jeu d'essai :\n",
    "\n",
    "2 films grand succès dans 2 genres différents : star wars et notting hill   \n",
    "un documentaire   \n",
    "un film en japonais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_films = [236, 1155, 2323, 3770]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Je définis une fonction helper pour l'affichage des résultats de recommandation à partir du jeu d'essai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# reçoit un array en entrée\n",
    "def print_films_by_position (films_position) :\n",
    "    df_films = df.iloc[films_position]\n",
    "    columns = ['movie_title', 'genres', 'budget', 'content_rating', \\\n",
    "               'director_name', 'actors', 'plot_keywords', 'country', 'language', 'title_year']\n",
    "    display(HTML(df_films[columns].to_html()))\n",
    "    \n",
    "def print_films_by_index (films_index) :\n",
    "    df_films = df.loc[films_index]\n",
    "    columns = ['movie_title', 'genres', 'budget', 'content_rating', \\\n",
    "               'director_name', 'actors', 'plot_keywords', 'country', 'language', 'title_year']\n",
    "    display(HTML(df_films[columns].to_html()))\n",
    "    \n",
    "def knn_results_positions(film_index) :\n",
    "    film_position = df.index.get_loc(film_index)\n",
    "    recommended_films_positions = indices[film_position].tolist()\n",
    "    return recommended_films_positions\n",
    "\n",
    "    \n",
    "def print_knn_results_essai(user_films_index) :\n",
    "    for film_index in user_films_index :   \n",
    "        film_position = df.index.get_loc(film_index)\n",
    "        print (\"user film : \")\n",
    "        print_films_by_position([film_position])\n",
    "        recommended_films_positions = knn_results_positions(film_index)\n",
    "        print (\"recommended films : \")\n",
    "        print_films_by_position(recommended_films_positions)\n",
    "\n",
    "        \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_results_index(film_index) :\n",
    "    film_position = df.index.get_loc(film_index)\n",
    "    film_cluster = labels[film_position]\n",
    "    recommended_films_positions = ([i for i in range(len(labels)) if labels[i] == film_cluster])\n",
    "    recommended_films_index = df.iloc[recommended_films_positions[:5]].index\n",
    "    return recommended_films_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clustering_results_essai(user_films_index) :\n",
    "    for film_index in user_films_index :   \n",
    "        film_position = df.index.get_loc(film_index)\n",
    "        print (\"user film : \")\n",
    "        print_films_by_position([film_position])\n",
    "        recommended_films_index = clustering_results_index(film_index)\n",
    "        print (\"recommended films : \")\n",
    "        print_films_by_index(recommended_films_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection de variables pour la modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4917, 8804)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "X = scipy.sparse.hstack([encoded_director,encoded_actors, encoded_genre, encoded_country, encoded_language, df_numeric]).todense()\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df.to_csv('X_df_sans_keywords_nb_variables_' + str(X_df.shape[1]) + '.csv')\n",
    "X_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components must be < n_features; got 9000 >= 8804",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f6181a86c6b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0msvd_explained_variance_ratio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 raise ValueError(\"n_components must be < n_features;\"\n\u001b[1;32m--> 174\u001b[1;33m                                  \" got %d >= %d\" % (k, n_features))\n\u001b[0m\u001b[0;32m    175\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0;32m    176\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components must be < n_features; got 9000 >= 8804"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_components_parameters = [500, 1000, 1500, 2000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "svd_explained_variance_ratio = []\n",
    "\n",
    "for n_components in n_components_parameters :\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components, n_iter=7, random_state=42)\n",
    "    svd.fit(X)  \n",
    "    svd_explained_variance_ratio.append(svd.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = calendar.timegm(time.gmtime())\n",
    "\n",
    "import simplejson\n",
    "f = open('svd_explained_variance_ratio_sans_keywords_pas_de_filtre' + str(ts), 'w')\n",
    "simplejson.dump(svd_explained_variance_ratio, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "\n",
    "ax = sns.lineplot(n_components_parameters, svd_explained_variance_ratio, color = 'b')\n",
    "ax.set_title('variance expliquée cumulée')\n",
    "ax.set_ylabel('variance expliquée cumulée')\n",
    "ax.set_xlabel('nb dimensions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle 1 : Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "ts = calendar.timegm(time.gmtime())\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=6).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X_df)\n",
    "np.save(indices, 'knn_baseline_indices_sans_keywords_sans_filtre' + str(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Test Recommandation, cas Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_knn_results_essai(user_films_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle 2 : K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "km = KMeans(n_clusters=100)\n",
    "km_labels = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.distplot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Test des recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_clustering_results_essai(user_films_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) \n",
    "# and the mean nearest-cluster distance (b) for each sample. \n",
    "# Silhouette Coefficient for a sample is (b - a) / max(a, b).\n",
    "\n",
    "#The best value is 1 and the worst value is -1. \n",
    "# Values near 0 indicate overlapping clusters. \n",
    "# Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "# http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import calendar;\n",
    "import time;\n",
    "ts = calendar.timegm(time.gmtime())\n",
    "print(ts)\n",
    "# 1540767599\n",
    "\n",
    "grid = [{'p1_n_frequency': [1,2,6], 'p2_n_clusters': [100, 200, 300,400], 'p3_model': ['km','ag']}]\n",
    "#grid = [{'p1_n_frequency': [1, 6, 30], 'p2_n_clusters': [100, 300, 400, 1000], 'p3_model': ['km'], 'p4_svd : []'}]\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_initial = pd.read_csv('df_cleaned.csv', index_col = 'index')\n",
    "\n",
    "estimator_scores = []\n",
    "nb_features = []\n",
    "sizes = []\n",
    "for g in ParameterGrid(grid) :\n",
    "    df = df_initial.copy()\n",
    "    model = g['p3_model']\n",
    "    if model == 'km' :\n",
    "        estimator = KMeans()\n",
    "    elif model == 'ag' :\n",
    "        estimator = AgglomerativeClustering()\n",
    "    n_clusters = g['p2_n_clusters']\n",
    "    estimator.set_params(**{'n_clusters': n_clusters})\n",
    "    n_frequency = g['p1_n_frequency']\n",
    "    encoded_actors = pre_process_actors(n_frequency)\n",
    "    encoded_director = pre_process_director(n_frequency)\n",
    "    encoded_keywords = pre_process_keywords(n_frequency)    \n",
    "    encoded_country = pre_process_country(n_frequency)\n",
    "    encoded_language = pre_process_language(n_frequency)\n",
    "    pre_process_content_rating(n_frequency)\n",
    "    df_numeric = pre_process_numeric_variables()\n",
    "    X = scipy.sparse.hstack([encoded_director,encoded_actors, encoded_genre, encoded_country, encoded_language]).todense()\n",
    "    estimator.fit(X)\n",
    "    estimator_labels = estimator.labels_\n",
    "    estimator_score = silhouette_score(X, estimator_labels)\n",
    "    estimator_scores.append([model, n_clusters, n_frequency, estimator_score])\n",
    "    nb_features.append(df.shape[1])\n",
    "    cluster_labels = np.unique(estimator.labels_)\n",
    "    for cluster in cluster_labels :\n",
    "        count = len(estimator_labels[estimator_labels == cluster])\n",
    "        sizes.append([model, n_clusters, n_frequency, count])\n",
    "\n",
    "    df.assign(label = estimator_labels)\n",
    "    filename = 'df_clustering_results_sans_coeff_sans_keywords_' + str(n_frequency) + \"_\" + str(n_clusters) + \"_\" + str(model) + \".csv\"\n",
    "    df.to_csv(filename, index_label = 'id')\n",
    "    print ('n_frequency : ' + str(n_frequency) + ', n_clusters : ' + str(n_clusters) + ' , model : ' + str(model)  + ' - nb de features = ' + str(X.shape[1]) +  ' - silhouette_score = ' + str(estimator_score))\n",
    "    \n",
    "np.save('estimator_scores' + '_' + str(ts), estimator_scores)\n",
    "np.save('nb_features'+ '_' + str(ts), nb_features)\n",
    "np.save('sizes'+ '_' + str(ts), sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.load('sizes_1541892709.npy')\n",
    "df_boxplot = pd.DataFrame(sizes, columns = ['model', 'n_clusters', 'n_frequency','size'])\n",
    "df_boxplot = df_boxplot[df_boxplot['n_frequency'] == '6']\n",
    "df_boxplot = df_boxplot.drop(columns = ['n_frequency'])\n",
    "df_boxplot['n_clusters'] = df_boxplot['n_clusters'].apply( lambda x : int(x))\n",
    "df_boxplot['size'] = df_boxplot['size'].apply(lambda x: int(x))\n",
    "\n",
    "sizes = np.load('estimator_scores_1541892709.npy')\n",
    "df_lineplot = pd.DataFrame(sizes, columns = ['model', 'n_clusters', 'n_frequency','score'])\n",
    "df_lineplot = df_lineplot[df_lineplot['n_frequency'] == '6']\n",
    "df_lineplot = df_lineplot.drop(columns = ['n_frequency'])\n",
    "df_km_lineplot = df_lineplot[df_lineplot['model'] == 'km']\n",
    "df_ag_lineplot = df_lineplot[df_lineplot['model'] == 'ag']\n",
    "parameters = df_ag_lineplot['n_clusters'].values.astype(np.float)\n",
    "km_scores = df_km_lineplot ['score'].values.astype(np.float)\n",
    "ag_scores = df_ag_lineplot['score'].values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax =plt.subplots(1,2, figsize=(20,10))\n",
    "ax[0].set_title('Coefficient Silhouette', fontsize=14)\n",
    "ax[0].set_ylabel('Coefficient Silhouette', fontsize=14)\n",
    "ax[0].set_xlabel('nb de clusters', fontsize=14)\n",
    "ax[1].set_title('Taille des clusters', fontsize=14)\n",
    "ax[1].set_ylabel('Taille des clusters', fontsize=14)\n",
    "ax[1].set_xlabel('nb de clusters', fontsize=14)\n",
    "\n",
    "sns.lineplot(parameters, km_scores, color = 'b', ax=ax[0], ci=None)\n",
    "sns.lineplot(parameters, ag_scores, color = 'orange', ax=ax[0], ci=None)\n",
    "sns.boxplot(y='size', x='n_clusters', hue='model',data=df_boxplot,  showfliers = False,  ax=ax[1], )\n",
    "#ax[1].set_yscale(\"log\")\n",
    "#sns.boxplot(y='size', x='n_clusters', hue='model',data=df_boxplot, ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avec le modèle Agglomerative clustering, le coefficient de silhouette est meilleur et la taille des clusters est plus homogène.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comparaison SVD et filtrage des valeurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_initial = pd.read_csv('df_cleaned.csv', index_col = 'index')\n",
    "\n",
    "estimator_scores = []\n",
    "nb_features = []\n",
    "sizes = []\n",
    "model == 'ag'\n",
    "estimator = AgglomerativeClustering(n_clusters= 300\n",
    "n_frequency = 0\n",
    "encoded_actors = pre_process_actors(n_frequency)\n",
    "encoded_director = pre_process_director(n_frequency)\n",
    "encoded_keywords = pre_process_keywords(n_frequency)    \n",
    "encoded_country = pre_process_country(n_frequency)\n",
    "encoded_language = pre_process_language(n_frequency)\n",
    "pre_process_content_rating(n_frequency)\n",
    "df_numeric = pre_process_numeric_variables()\n",
    "X = scipy.sparse.hstack([encoded_director,encoded_actors, encoded_genre, encoded_country, encoded_language]).todense()\n",
    "estimator.fit(X)\n",
    "estimator_labels = estimator.labels_\n",
    "estimator_score = silhouette_score(X, estimator_labels)\n",
    "estimator_scores.append([model, n_clusters, n_frequency, estimator_score])\n",
    "nb_features.append(df.shape[1])\n",
    "cluster_labels = np.unique(estimator.labels_)\n",
    "for cluster in cluster_labels :\n",
    "    count = len(estimator_labels[estimator_labels == cluster])\n",
    "    sizes.append([model, n_clusters, n_frequency, count])\n",
    "\n",
    "df.assign(label = estimator_labels)\n",
    "filename = 'df_clustering_results_svd_' + str(n_frequency) + \"_\" + str(n_clusters) + \"_\" + str(model) + \".csv\"\n",
    "df.to_csv(filename, index_label = 'id')\n",
    "print ('n_frequency : ' + str(n_frequency) + ', n_clusters : ' + str(n_clusters) + ' , model : ' + str(model)  + ' - nb de features = ' + str(X.shape[1]) +  ' - silhouette_score = ' + str(estimator_score))\n",
    "    \n",
    "np.save('estimator_scores_svd' + '_' + str(ts), estimator_scores)\n",
    "np.save('nb_features_svd'+ '_' + str(ts), nb_features)\n",
    "np.save('sizes_svd'+ '_' + str(ts), sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle retenu\n",
    "**Agglomerative clustering avec 300 clusters, après filtrage des valeurs utilisées pour au moins 6 films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_csv('df_cleaned.csv', index_col = 'index')\n",
    "df = df_initial.copy()\n",
    "\n",
    "estimator_scores = []\n",
    "nb_features = []\n",
    "sizes = []\n",
    "model == 'ag'\n",
    "n_frequency = 0\n",
    "n_clusters = 300\n",
    "estimator = AgglomerativeClustering(n_clusters= n_clusters)\n",
    "encoded_actors = pre_process_actors(n_frequency)\n",
    "encoded_director = pre_process_director(n_frequency)\n",
    "encoded_keywords = pre_process_keywords(n_frequency)    \n",
    "encoded_country = pre_process_country(n_frequency)\n",
    "encoded_language = pre_process_language(n_frequency)\n",
    "pre_process_content_rating(n_frequency)\n",
    "df_numeric = pre_process_numeric_variables()\n",
    "X = scipy.sparse.hstack([encoded_director,encoded_actors, encoded_genre, encoded_country, encoded_language]).todense()\n",
    "estimator.fit(X)\n",
    "estimator_labels = estimator.labels_\n",
    "estimator_score = silhouette_score(X, estimator_labels)\n",
    "estimator_scores.append([model, n_clusters, n_frequency, estimator_score])\n",
    "nb_features.append(df.shape[1])\n",
    "cluster_labels = np.unique(estimator.labels_)\n",
    "for cluster in cluster_labels :\n",
    "    count = len(estimator_labels[estimator_labels == cluster])\n",
    "    sizes.append([model, n_clusters, n_frequency, count])\n",
    "\n",
    "df.assign(label = estimator_labels)\n",
    "filename = 'df_clustering_results_sans_coeff_sans_keywords_' + str(n_frequency) + \"_\" + str(n_clusters) + \"_\" + str(model) + \".csv\"\n",
    "df.to_csv(filename, index_label = 'id')\n",
    "print ('n_frequency : ' + str(n_frequency) + ', n_clusters : ' + str(n_clusters) + ' , model : ' + str(model)  + ' - nb de features = ' + str(X.shape[1]) +  ' - silhouette_score = ' + str(estimator_score))\n",
    "    \n",
    "np.save('estimator_scores' + '_' + str(ts), estimator_scores)\n",
    "np.save('nb_features'+ '_' + str(ts), nb_features)\n",
    "np.save('sizes'+ '_' + str(ts), sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_films_index = [523,236, 1155, 2323, 1224, 344, 1065]\n",
    "labels = estimator.labels_\n",
    "print_clustering_results_essai(user_films_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise au format API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Enregistrement du résultat de la modélisation, cas clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(label = ag_labels)\n",
    "df.to_csv('df_clustering_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Enregistrement du résultat de la modélisation, cas clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(indices = indices)\n",
    "df.to_csv('df_neighbors_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction à recopier dans l'API\n",
    "\n",
    "# récupérer les résultats de la modélisation\n",
    "results = pd.read_csv('df_clustering_results.csv')\n",
    "\n",
    "import simplejson as json\n",
    "\n",
    "def knn_results_index(film_index) :\n",
    "    film_position = df.index.get_loc(film_index)\n",
    "    recommended_films_positions = indices[film_position].tolist()\n",
    "    recommended_films_index = df.iloc[recommended_films_positions].index.tolist()\n",
    "    return recommended_films_index\n",
    "\n",
    "def clustering_results_index(film_index) :\n",
    "    film_position = df.index.get_loc(film_index)\n",
    "    film_cluster = df.iloc[[film_position]]['label']\n",
    "    recommended_films = df[df['label'] == film_cluster]]\n",
    "    recommended_films_index = recommended_films.index.tolist()\n",
    "    return recommended_films_index  \n",
    "\n",
    "# afficher les recommandations\n",
    "def return_json_recommended_films(film_index) :\n",
    "    film_index = int(film_index)\n",
    "    # récupérer les résultats du modèle\n",
    "    # si clustering\n",
    "    recommended_films_index = clustering_results_index(film_index)\n",
    "    # si knn\n",
    "    #recommended_films_index = knn_results_index(film_index)\n",
    "    \n",
    "    results = []\n",
    "    for index in recommended_films_index :\n",
    "        dict_film = {'id' : index, 'name' : df.loc[[index]]['movie_title'].values[0].split(\"\\xa0\")[0] }\n",
    "        results.append(dict_film)\n",
    "    dict_results = {'_results': results }\n",
    "    print(json.dumps(dict_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérification de l'API\n",
    "film_index = 2364\n",
    "return_json_recommended_films(film_index)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
